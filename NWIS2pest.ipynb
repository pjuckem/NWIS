{
 "metadata": {
  "name": "",
  "signature": "sha256:e4e20aa95387d7e41cb368d7aee040a71782d3a3eaefdca8bfecd8ab5382cd07"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Program to QC groundwater levels from NWIS database and generate mod2obs file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Required Inputs:\n",
      "list of counties with NWIS Site Files and level files generated by \"pullNWISdata.py.\"\n",
      "Required NWIS fields: site_no, alt_va, alt_acy_va, lev_dt, lev_va, lev_status_cd, well_depth_va, ....others\n",
      "\n",
      "Output:  Mod2Obs files, PEST INS file, and portion of PST file with head obs, plus a shapefile.\n",
      "\n",
      "Considerations and ToDo:\n",
      "1.  Nested wells will have reduced weight as part of the declustering process.  For FWP, this is desirable, as a consistent set of\n",
      "targets is required for comparison.  A subroutine could potentially be added to handle nested wells differently, by applying the\n",
      "declustering algorithm after assigning to layers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import datetime as dt\n",
      "import geopandas as gpd\n",
      "import pandas as pd\n",
      "from fiona import crs\n",
      "from shapely.geometry import Point, LineString, Polygon\n",
      "import sys\n",
      "import os\n",
      "import flopy\n",
      "import pysal as ps\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "if sys.platform == 'darwin' or 'nix' in sys.platform:\n",
      "    newline = '\\r\\n'\n",
      "else:\n",
      "    newline = '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Setup section"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Outfiles\n",
      "outpath = 'D:/PFJData2/Projects/NAQWA/Cycle3/FWP/MODFLOW/PESTsetup/'\n",
      "outshape = 'D:/PFJData2/Projects/NAQWA/Cycle3/FWP/ARC/PEST/targets/NWIS_targets.shp'\n",
      "\n",
      "# Input files\n",
      "calib_area = 'D:/PFJData2/Projects/NAQWA/Cycle3/FWP/ARC/PEST/FluxTargetHUCS.shp'\n",
      "mfpath = 'D:/PFJData2/Projects/NAQWA/Cycle3/FWP/MODFLOW/working'\n",
      "mfnam = 'FWPvert_uniformall.nam'\n",
      "MFdis = 'FWPvert.dis'\n",
      "origin = '826111.706, 15791723.564'  # origin or lower left of model grid in world coordinates of the model (UTM-ft)\n",
      "uprleft = '916863.802, 16717285.046'\n",
      "datadir = 'D:/PFJData2/Projects/NAQWA/Cycle3/FWP/MODFLOW/PESTsetup/'\n",
      "infofileprefix = 'mwell_siteinfo_'\n",
      "levelfileprefix = 'mwell_dtw_'\n",
      "countylistfile = 'D:/PFJData2/Projects/NAQWA/Cycle3/FWP/MODFLOW/PESTsetup/countylist.dat'\n",
      "NAWQAwells = 'D:\\PFJData2\\Projects\\NAQWA\\C3GIS\\All_sites.shp'\n",
      "#countylist = ['137']  # note: 141 = wood county = some wells in;some out\n",
      "\n",
      "# MOD2OBS input:  *.spc and *.fig exported from GWvistas.  Need to update for multi-layer models.\n",
      "driverfile = 'mod2obs_FWP.in'\n",
      "bore_coords_file = 'FWP_bore_coords.crd'\n",
      "bore_list_file = 'FWP_bore_list.crd'\n",
      "well_sample_file = 'FWP_GWSI.smp'\n",
      "hds_file = 'FWPvert_uniformall.hds'\n",
      "insfile = 'FWP_GWSI.ins'\n",
      "pest_head_obs_sect_file = 'FWP_pst_GWSI.dat'\n",
      "spc_file = 'FWPvert.spc'\n",
      "flow_or_trans = 'f'\n",
      "inactive_thresh = 1.0e+30\n",
      "time_units = 'd'\n",
      "nlay = 1\n",
      "extrapolation_limit = 1.0e+10\n",
      "output_file = 'FWP_uniformall_GWSI.smp'\n",
      "ss_arbitrary_date = '12/20/2012'\n",
      "ss_arbitrary_time = '12:00:00'\n",
      "\n",
      "# Settings\n",
      "glacial_networks = ['glacetn1', 'glacfps1', 'glacpas1', 'wmicfpsag2a', 'wmicfpsag2b', 'wmiclusag1a', 'wmiclusag1b',\n",
      "                  'wmiclusag2', 'wmicreffo1', 'wmicsus2', 'glacmss1']\n",
      "glac_only = True\n",
      "decluster = True\n",
      "decluster_distance = 10000  # units of the projection; 10x the cell size for FWP\n",
      "start = dt.datetime(1970, 1, 1)  # start and end dates for evaluating WL data\n",
      "end = dt.datetime(2013, 12, 31)\n",
      "min_msmts = 2  # minimum number of wl msmts in NWIS to be included as a target (eliminates pre-1980 WCRs)\n",
      "onlysubft = True  # another filter to retain only reasonably decent wells.\n",
      "# A set of dictionaries to evaluate the accuracy of individual msmts. values are in feet.\n",
      "# see: http://pubs.usgs.gov/of/2004/1238/\n",
      "lev_errdict = {0:1.01, 1:0.1, 2:0.01, 9:10}  # direct estimate of msmt accuracy.\n",
      "src_errdict = {'A':0.01,'D':5,'G':0.01,'L':0.1,'M':10,'O':5,'R':.1,'S':0.01,'Z':.1}  # agency msmts likely accurate; Drillers, owners, other likely from time of drilling\n",
      "meth_errdict = {'A':1.01,'B':0.01,'C':0.01,'E':5,'F':0.01,'G':0.01,'H':0.01,'L':0.1,'M':0.01,'N':0.1,'R':1.01,'S':0.01,'T':0.01,'U':.1,'V':0.01,'Z':1.01}  # method accuracy.\n",
      "# list of level_status_cds to discard. EG: if well was recently pumped, discard the msmt (but not well).(using ALL codes)\n",
      "discard_lev_status_cd=['A','B','D','E','F','G','H','I','J','M','N','O','P','R','S','T','V','W','X','Z']\n",
      "# list of coordinate_codes to keep. Keeping only wells with locations better or equal to 5 degree-seconds (F)\n",
      "#keep_coord_cd = {'H', '1', '5', 'S', 'R', 'F'}  # might want to add 'T' (10 degree-seconds, or about 900ft latitude)\n",
      "# keep_coords_cd now removed b/c many long-term wells are coded as 1-minute accuracy, but is likely better than that.\n",
      "\n",
      "# misc variables\n",
      "driverfile = os.path.join(outpath, driverfile)\n",
      "bore_coords_file = os.path.join(outpath, bore_coords_file)\n",
      "bore_list_file = os.path.join(outpath, bore_list_file)\n",
      "well_sample_file = os.path.join(outpath, well_sample_file)\n",
      "hds_file = os.path.join(outpath, hds_file)\n",
      "insfile = os.path.join(outpath, insfile)\n",
      "pest_head_obs_sect_file = os.path.join(outpath, pest_head_obs_sect_file)\n",
      "spc_file = os.path.join(outpath, spc_file)\n",
      "output_file = os.path.join(outpath, output_file)\n",
      "UTM83Z16_ft = {u'proj':u'utm', u'zone':16, u'datum':u'NAD83', u'units':u'us-ft', u'no_defs':True}  # UTM83 zone 16 feet\n",
      "#USGSalbers = {u'proj':u'aea', u'datum':u'NAD83', u'lon_0':-96.0 , u'lat_0':23, u'lat_1':29.5, u'lat_2':45.5,\n",
      "#              u'x_0':0, u'y_0':0, u'units':u'm', u'no_defs':True}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Functions from: http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n",
      "def unit_vector(vector):\n",
      "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
      "    return vector / np.linalg.norm(vector)\n",
      "\n",
      "def angle_between(v1, v2):\n",
      "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2':\n",
      "            >> angle_between((1, 0, 0), (0, 1, 0))\n",
      "            1.5707963267948966\n",
      "            >> angle_between((1, 0, 0), (1, 0, 0))\n",
      "            0.0\n",
      "            >> angle_between((1, 0, 0), (-1, 0, 0))\n",
      "            3.141592653589793\n",
      "    \"\"\"\n",
      "    v1_u = unit_vector(v1)\n",
      "    v2_u = unit_vector(v2)\n",
      "    angle = np.arccos(np.dot(v1_u, v2_u))\n",
      "    if np.isnan(angle):\n",
      "        if (v1_u == v2_u).all():\n",
      "            return 0.0\n",
      "        else:\n",
      "            return np.pi\n",
      "    return angle\n",
      "\n",
      "def angle(orig, point1, point2, unit='deg'):\n",
      "    \"\"\"  Input is 2 or 3 x,y coordinate pairs supplied as arrays. \"\"\"\n",
      "    v1 = point1 - orig\n",
      "    v2 = point2 - orig\n",
      "    rad_angle = np.array([angle_between(v1, v2)])\n",
      "    if unit == 'deg':\n",
      "        deg_angle = np.degrees(rad_angle)\n",
      "        return deg_angle\n",
      "    elif unit == 'rad':\n",
      "        return rad_angle\n",
      "    else:\n",
      "        print 'Error.  Please specify \"deg\" for degrees or \"rad\" for radians'\n",
      "\n",
      "def xy2np(string):\n",
      "    \"\"\"\n",
      "    Convert a string of x and y into a numpy array vector of x & y\n",
      "    \"\"\"\n",
      "    x,y = string.split(',')\n",
      "    x = float(x)\n",
      "    y = float(y)\n",
      "    xy = np.array([x, y])\n",
      "    return xy\n",
      "\n",
      "def calc_angle(str_orig, string1, string2=None, unit='deg'):\n",
      "    \"\"\"\n",
      "    Wrap-up all of the above into a one-liner.\n",
      "    If string2 is not provided, assume point2 is on y-axis and desired angle is from pos y-axis (from north).\n",
      "    \"\"\"\n",
      "    orig = xy2np(str_orig)\n",
      "    point1 = xy2np(string1)\n",
      "    if string2:\n",
      "        point2 = xy2np(string2)\n",
      "    else:\n",
      "        point2 = orig + np.array([0,10])\n",
      "    arr_angle = angle(orig, point1, point2, unit)\n",
      "    return arr_angle # return array\n",
      "\n",
      "# get header info\n",
      "def getheader(filename,indicator,delimiter):\n",
      "    headervar=0\n",
      "    infile=open(filename,'r').readlines()\n",
      "    for line in infile:\n",
      "        cline=line.strip().split(delimiter)\n",
      "        if cline[0]==indicator:\n",
      "            break\n",
      "        else:\n",
      "            headervar+=1\n",
      "    return(headervar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Main Code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Read-in a list of NAWQA well networks.  Don't remove wells that are part of these networks at anypoint along\n",
      "the QA/QC process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"getting list of NAWQA wells\"\n",
      "# Use Geopandas to clip full NAWQA well database to calibration area.\n",
      "# Hack to process the returned bool series (there has to be a better way...)\n",
      "# Note: can't use arcpy.clip because object dtype columns get dropped, such as station name.\n",
      "calib = gpd.read_file(calib_area)\n",
      "geom = calib.geometry\n",
      "\n",
      "# keep all NAWQA wells in the provided list of glacial networks.\n",
      "nawqa = gpd.read_file(NAWQAwells)\n",
      "\n",
      "usgsalbers = nawqa.crs\n",
      "nawqa.to_crs(crs=UTM83Z16_ft, inplace=True)\n",
      "\n",
      "keep = [geom.contains(pt) for pt in nawqa['geometry']]\n",
      "lst = []\n",
      "for r, ser in enumerate(keep):\n",
      "    val = str(keep[r]).split()[1]\n",
      "    if val == 'True': val = True\n",
      "    elif val == 'False': val = False\n",
      "    else:\n",
      "        print \"error\"\n",
      "        break\n",
      "    lst.append(val)\n",
      "nawqa.loc[:,'keep'] = lst\n",
      "nawqa = nawqa.loc[nawqa['keep']==True, :]  # remove wells outside the calibration area from the dataframe\n",
      "nawqa = nawqa.loc[nawqa['SuCode'].isin(glacial_networks), :]  # keep only wells in the glacial aquifer networks\n",
      "lst = nawqa['SITE_NO'].tolist()\n",
      "nawqalst = [int(id) for id in lst]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Pull-in the datafiles from NWIS that were produced from the pullNWISdata.py script."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"getting all well info, water levels, and coordinates per county from NWIS...\"\n",
      "countyfile = open(countylistfile, 'r')\n",
      "countylist = countyfile.readline().split(',')\n",
      "countylist = ''.join(countylist).split()  # convert to str, remove whitespaces\n",
      "n = 0\n",
      "for c in countylist:\n",
      "    infofile = os.path.join(datadir, (infofileprefix + c + '.dat'))\n",
      "    levelsfile = os.path.join(datadir, (levelfileprefix + c + '.dat'))\n",
      "    \n",
      "    info_header=getheader(infofile,'agency_cd','\\t')\n",
      "    levels_header=getheader(levelsfile,'agency_cd','\\t')\n",
      "    \n",
      "    if n==0:\n",
      "        info=np.genfromtxt(infofile,delimiter='\\t',skiprows=info_header,names=True,dtype=None, comments='garbalygook')[1:]  # don't use # as default comment -- it's in some well names\n",
      "        siteinfodf = pd.DataFrame(info)\n",
      "        levelsdata=np.genfromtxt(levelsfile,delimiter='\\t',skiprows=levels_header,names=True,dtype=None)[1:]\n",
      "        levelsdf = pd.DataFrame(levelsdata)\n",
      "    else:\n",
      "        info1=np.genfromtxt(infofile,delimiter='\\t',skiprows=info_header,names=True,dtype=None, comments='garbalygook')[1:]\n",
      "        siteinfo1 = pd.DataFrame(info1)\n",
      "        siteinfodf = siteinfodf.append(siteinfo1)\n",
      "        levelsdata1=np.genfromtxt(levelsfile,delimiter='\\t',skiprows=levels_header,names=True,dtype=None)[1:]\n",
      "        levels1 = pd.DataFrame(levelsdata1)\n",
      "        levelsdf = levelsdf.append(levels1)\n",
      "    n += 1\n",
      "\n",
      "# convert to dataframes and set site_no as the index\n",
      "siteinfodf.site_no = siteinfodf.site_no.astype(long)\n",
      "siteinfodf.index = siteinfodf['site_no']\n",
      "levelsdf['site_no'] = levelsdf['site_no'].astype(long)\n",
      "levelsdf.index = levelsdf['site_no']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Clean-up the data fields"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Clean-up fields and change dtype so can aggregate\n",
      "#  This function doesn't work. Needs more work. Commands work when not part of a function...\n",
      "def clean_to_floats(df, fieldlist):\n",
      "    for field in fieldlist:\n",
      "        df = df.loc[df[field] != ('\\D+'), :] # remove rows with non-float values.\n",
      "        df = df.loc[df[field] != (''), :] # remove empty rows (ideas to add this to line above?).\n",
      "        df[field] = df[field].astype(float)\n",
      "#siteselectcols = ['dec_lat_va', 'dec_long_va', 'alt_va', 'alt_acy_va', 'well_depth_va']\n",
      "#levelsselectcols = ['lev_va', 'lev_acy_cd']\n",
      "#clean_to_floats(siteinfodf, siteselectcols)\n",
      "#clean_to_floats(levelsdf, levelsselectcols)\n",
      "\n",
      "# Do it manually instead...\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['well_depth_va']!=('\\D+'), :]  # remove rows with non-float values.\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['well_depth_va']!=(''), :]  # don't keep blank rows\n",
      "siteinfodf.well_depth_va = siteinfodf.well_depth_va.astype(float)\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['dec_lat_va']!=('\\D+'), :]\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['dec_lat_va']!=(''), :]\n",
      "siteinfodf.dec_lat_va = siteinfodf.dec_lat_va.astype(float)\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['dec_long_va']!=('\\D+'), :]\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['dec_long_va']!=(''), :]\n",
      "siteinfodf.dec_long_va = siteinfodf.dec_long_va.astype(float)\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['alt_va']!=('\\D+'), :]\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['alt_va']!=(''), :]\n",
      "siteinfodf.alt_va = siteinfodf.alt_va.astype(float)\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['alt_acy_va']!=('\\D+'), :]\n",
      "siteinfodf = siteinfodf.loc[siteinfodf['alt_acy_va']!=(''), :]\n",
      "siteinfodf.alt_acy_va = siteinfodf.alt_acy_va.astype(float)\n",
      "levelsdf = levelsdf.loc[levelsdf['lev_va']!=('\\D+'), :]\n",
      "levelsdf = levelsdf.loc[levelsdf['lev_va']!=(''), :]\n",
      "levelsdf.lev_va = levelsdf.lev_va.astype(float)\n",
      "levelsdf = levelsdf.loc[levelsdf['lev_acy_cd']!=('\\D+'), :]\n",
      "levelsdf = levelsdf.loc[levelsdf['lev_acy_cd']!=(''), :]\n",
      "levelsdf.lev_acy_cd = levelsdf.lev_acy_cd.astype(float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Filter the dataset, evaluate accuracy codes, compute average WLs only remove non-NAWQA wells"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "levelsdf = levelsdf.copy().join(siteinfodf.loc[:, ['alt_acy_va']])  # join based on the indexes\n",
      "naw = levelsdf['site_no'].isin(nawqalst)\n",
      "levkeep = ~levelsdf['lev_status_cd'].isin(discard_lev_status_cd)  # keep msmts w/out errors. Note: the \"~\"\n",
      "keeps = naw | levkeep  # keep msmts w/out errors...unless are NAWQA wells (must keep all NAWQA wells)\n",
      "levelsdf = levelsdf.loc[keeps]\n",
      "#levelsdf = (levelsdf.loc[~levelsdf['lev_status_cd'].isin(discard_lev_status_cd), :] | levelsdf.loc[~levelsdf['site_no'].isin(nawqalst), :])  # memory error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add columns then decode the dictionaries\n",
      "levelsdf['src_err'] = levelsdf['lev_src_cd'].copy()\n",
      "levelsdf['meth_err'] = levelsdf['lev_meth_cd'].copy()\n",
      "levelsdf['levacy_err'] = levelsdf['lev_acy_cd'].copy()\n",
      "levelsdf = levelsdf.replace({'src_err':src_errdict}).copy()\n",
      "levelsdf = levelsdf.replace({'meth_err':meth_errdict}).copy()\n",
      "levelsdf = levelsdf.replace({'levacy_err':lev_errdict}).copy()\n",
      "levelsdf['cum_err'] = levelsdf['levacy_err'] + levelsdf['meth_err'] + levelsdf['src_err']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove wells with really poor measurements\n",
      "if onlysubft:\n",
      "    print \"limiting wells to those with cumulative measurement errors < 1 ft\"\n",
      "    # retains only decent USGS wells.\n",
      "    okerr = levelsdf['cum_err'] < 1.0\n",
      "    naw = levelsdf['site_no'].isin(nawqalst)  # must redo b/c levelsdf was purged after previous setting of this bool array.\n",
      "    keeps = naw | okerr\n",
      "    levelsdf = levelsdf.loc[keeps]\n",
      "    #levelsdf = (levelsdf[(levelsdf['cum_err'] < 1.0)] | levelsdf.loc[~levelsdf['site_no'].isin(nawqalst), :])  # memory error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove wells that haven't been measured enough (ie: remove WCRs -- those are better obtained from DNR)\n",
      "levelsdf['n_msmts'] = levelsdf.groupby(['site_no'])['lev_va'].count()\n",
      "#okacy = levelsdf['alt_acy_va'] <= 0.01  # this keeps wells that might otherwise be discarded (1 msmt), except those that were professionally surveyed\n",
      "oknmsmt = levelsdf['n_msmts'] >= min_msmts\n",
      "naw = levelsdf['site_no'].isin(nawqalst)\n",
      "#keeps = naw | okacy | oknmsmt  # keep msmts w/ OK accuracy OR min # msmts or is a NAWQA well\n",
      "keeps = naw | oknmsmt  # keep msmts w/ OK accuracy OR min # msmts or is a NAWQA well\n",
      "levelsdf = levelsdf.loc[keeps]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Limit by dates, compute ave WL.\n",
      "levelsdf['lev_dt'] = pd.to_datetime(levelsdf['lev_dt'])\n",
      "okstrt = levelsdf['lev_dt'] >= start\n",
      "okend = levelsdf['lev_dt'] <= end\n",
      "okdates = okstrt & okend\n",
      "naw = levelsdf['site_no'].isin(nawqalst)\n",
      "keeps = naw | okdates  # keep msmts w/ ok date range or a NAWQA well\n",
      "levelsdf = levelsdf.loc[keeps]\n",
      "\n",
      "levelsdf['first'] = levelsdf.groupby(['site_no'])['lev_dt'].min()\n",
      "levelsdf['last'] = levelsdf.groupby(['site_no'])['lev_dt'].max()\n",
      "tdiff = levelsdf['last'] - levelsdf['first']  # minimum of 1 day\n",
      "tdiff = tdiff.reset_index()\n",
      "tdiff.columns = ['site_no', 'diff']\n",
      "tdiff.index = tdiff['site_no']\n",
      "#tdiff['td'] = [1.157407 * 10**-14 * float(td) for td in tdiff['diff']]  # convert to float; convert to days.\n",
      "# This worked with NP v1.7 & PD v0.14, but broke after update to NP1.8 & PD0.15. wouldn't work when tried to revert.\n",
      "# Temporary hack while Pandas team works to better handle timedelta dtypes....\n",
      "tdiff['td'] = [float(str(td).split()[0]) for td in tdiff['diff']]  # Note: units are now presumed to be days rather than nanoseconds.\n",
      "tdiff2 = tdiff.groupby(tdiff.index).agg(np.mean)  # warning, the column 'site_no' gets changed\n",
      "tdiff2 = tdiff2.drop('site_no', axis=1)  # line above altered 'site_no' column (not index), so remove it for safety.\n",
      "\n",
      "lev2 = levelsdf.groupby('site_no').agg(np.mean)  # compute mean WL (all fields), then aggregates by site number\n",
      "lev2 = lev2.copy().join(tdiff2.loc[:])  # add time differences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Join site information with water level measurements"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Join-in site information\n",
      "lev3 = lev2.join(siteinfodf.loc[:, ['station_nm', 'county_cd', 'dec_lat_va', 'dec_long_va', 'coord_acy_cd',\n",
      "                                     'nat_aqfr_cd', 'aqfr_cd', 'aqfr_type_cd', 'alt_va', 'well_depth_va']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Limit to glacial wells."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Don't include BR wells, even for the model with BR layers.  Want to keep target dataset the same for all calibrations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Keep only glacial wells\n",
      "if glac_only:\n",
      "    print \"removing non-glacial wells\"\n",
      "    lev3 = lev3[((lev3['aqfr_cd'] == '100SDGV') | (lev3['nat_aqfr_cd'] == 'N100GLCIAL') | (lev3['aqfr_cd'] == '') |\n",
      "                  (lev3['aqfr_cd'] == '110QRNR') | (lev3['aqfr_cd'] == '110SDGVP') | (lev3['aqfr_cd'] == '110SANDP'))]\n",
      "    # Keep only glacial wells w/ accuracy of 5 seconds or better (BR can tolerate less accuracy due to flow sys detail)\n",
      "    #lev3 = lev3.loc[lev3['coord_acy_cd'].isin(keep_coord_cd), :]  # keep sites with location w/in +/- X seconds (F=5sec)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Clip targets to the calibration area (avoid boundary problems)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"re-projecting coordinates to UTM-ft, and clipping to the calibration area\"\n",
      "x, y = lev3['dec_long_va'], lev3['dec_lat_va']\n",
      "xy = zip(x, y)\n",
      "points = gpd.GeoSeries([Point(x, y) for x, y in xy])\n",
      "lev3 = lev3.reset_index()  # have to revert to a basic index to get points to match up\n",
      "lev3['geometry'] = points\n",
      "inputEPSG = 4326 # lat long\n",
      "crs = crs.from_epsg(inputEPSG)\n",
      "lev3 = gpd.GeoDataFrame(lev3, crs=crs)\n",
      "UTM83Z16_ft = {u'proj':u'utm', u'zone':16, u'datum':u'NAD83', u'units':u'us-ft', u'no_defs':True}  # UTM83 zone 16 feet\n",
      "lev3.to_crs(crs=UTM83Z16_ft, inplace=True)\n",
      "\n",
      "# Use Geopandas to determine which points are in/out.\n",
      "# Hack to process the returned bool series (there has to be a better way...)\n",
      "# Note: can't use arcpy.clip because object dtype columns get dropped, such as station name.\n",
      "calib = gpd.read_file(calib_area)\n",
      "geom = calib.geometry\n",
      "keep = [geom.contains(pt) for pt in lev3['geometry']]\n",
      "lst = []\n",
      "for r, ser in enumerate(keep):\n",
      "    val = str(keep[r]).split()[1]\n",
      "    if val == 'True': val = True\n",
      "    elif val == 'False': val = False\n",
      "    else:\n",
      "        print \"error\"\n",
      "        break\n",
      "    lst.append(val)\n",
      "lev3.loc[:,'keep'] = lst\n",
      "lev3 = lev3.loc[lev3['keep']==True, :]  # remove wells outside the calibration area from the dataframe\n",
      "lev3 = lev3.set_index('site_no')  # reset index after converting coordinates and clipping"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Truncate names for PEST, remove duplicate names"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Truncating names to 10-spaces for PEST\"\n",
      "labels = []\n",
      "names = lev3['station_nm'].tolist()\n",
      "for name in names:\n",
      "    last = name.split('/')[-1]\n",
      "    try:\n",
      "        last[10]  # if longer than 10 characters, truncate\n",
      "        last = last[-10:]\n",
      "    except:\n",
      "        pass\n",
      "    last = last.strip()\n",
      "    labels.append(last)\n",
      "lev3.loc[:,'labels'] = labels\n",
      "\n",
      "print \"     Checking for duplicate names and fixing them\"\n",
      "lev3['dup'] = lev3.duplicated(subset='labels')\n",
      "dups = lev3.copy()[lev3['dup'] == True]\n",
      "n = dups['dup'].count()\n",
      "while n > 0:\n",
      "    newlabels = []\n",
      "    names = dups['labels'].tolist()\n",
      "    for i, name in enumerate(names):\n",
      "        last = name[-7:]  # take the last 7 characters\n",
      "        last = str(i+1) + '_' + last  # add an index\n",
      "        newlabels.append(last)\n",
      "    dups.loc[:,'labels'] = newlabels\n",
      "    lev3.loc[:,'newlabels'] = dups.loc[:,'labels']\n",
      "    lev3['labels'] = np.where(lev3.loc[:, 'newlabels'].isnull(), lev3['labels'], lev3['newlabels'])  # .loc for bool series\n",
      "    lev3['dup'] = lev3.duplicated(subset='labels')\n",
      "    dups = lev3.copy()[lev3['dup'] == True]\n",
      "    n = dups['dup'].count()\n",
      "    lev3 = lev3.drop(['newlabels'], axis=1)\n",
      "\n",
      "print \"Removing spaces from all names.\"\n",
      "lev3['labels'] = lev3['labels'].str.replace(' ', '_')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Assign targets to layers based on well bottom info."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: NWIS doesn't have reliable casing depth info."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"assigning targets to layers\"\n",
      "m = flopy.modflow.Modflow(model_ws=mfpath)\n",
      "nf = flopy.utils.mfreadnam.parsenamefile(os.path.join(mfpath, mfnam), {})\n",
      "dis = flopy.modflow.ModflowDis.load(os.path.join(mfpath, MFdis), m, nf)\n",
      "tops = np.zeros((dis.nrow, dis.ncol))\n",
      "tops[:, :] = dis.top.array\n",
      "bots = np.zeros((dis.nlay, dis.nrow, dis.ncol))\n",
      "bots[:, :, :] = dis.botm.array\n",
      "nrow = dis.nrow\n",
      "ncol = dis.ncol\n",
      "\n",
      "lev3['wellelev'] = lev3['alt_va'] - lev3['well_depth_va']\n",
      "wellbotelev = np.array(lev3['wellelev'].tolist())\n",
      "rad_angle = calc_angle(origin, uprleft, unit='rad')  # radian_angle from true north\n",
      "geom = lev3['geometry'].tolist()\n",
      "x_orig = float(origin.split(',')[0])\n",
      "y_orig = float(origin.split(',')[1])\n",
      "xcoords = [geom[i].x for i,j in enumerate(geom)]\n",
      "ycoords = [geom[i].y for i,j in enumerate(geom)]\n",
      "#  compute row,col indexes for a rotated grid\n",
      "wellcolsidx = [int(np.floor((x - x_orig - (np.tan(rad_angle) * (ycoords[i] - y_orig)))/(1000.0 / np.cos(rad_angle)))) for i, x in enumerate(xcoords)]  # columns are 1000ft\n",
      "wellrowsidx = [nrow-1 - int(np.floor((y - y_orig + (np.tan(rad_angle) * (xcoords[i] - x_orig)))/(1000.0 / np.cos(rad_angle)))) for i, y in enumerate(ycoords)]\n",
      "\n",
      "layer = np.full_like(wellbotelev, -999)  # -999 error flag\n",
      "topatwells = np.array([tops[i,j] for i,j in zip(wellrowsidx, wellcolsidx)])\n",
      "for k in range(bots.shape[0]):\n",
      "    layKbotatwells = np.array([bots[k,i,j] for i,j in zip(wellrowsidx, wellcolsidx)])\n",
      "    layer = np.where(wellbotelev >= layKbotatwells, k+1, layer)  # designate the lowest layer the target can be in\n",
      "    layer = np.where(wellbotelev > topatwells, 999, layer)  # a flag if the well bot is above LS -- what's going on?\n",
      "minval = layer.min()\n",
      "if minval < 0:\n",
      "    print \"At least one target has a well bottom below the bottom of the model.  They will be removed as targets\"\n",
      "maxval = layer.max()\n",
      "if maxval >= 999.0:\n",
      "    layer = np.where(layer >= 999.0, 1, layer)\n",
      "    print \"At least one target has a well bottom above the model top.  They have been assigned to layer 1.\"\n",
      "\n",
      "lev3['layer'] = layer\n",
      "lev3['x'] = xcoords\n",
      "lev3['y'] = ycoords\n",
      "lev3['date'] = ss_arbitrary_date\n",
      "lev3['time'] = ss_arbitrary_time\n",
      "lev3['target'] = lev3['alt_va'] - lev3['lev_va']\n",
      "lev3 = lev3[(lev3['layer'] != -999)]  # removing wells below the model bottom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Assigning weights & groups"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tried a few approaches for weighting in hopes of using an equation, but binning seemed more straightforward in the end."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "print \"assigning weights & target groups\"\n",
      "lev2['ini_weight'] = -1.0\n",
      "# Compute weight as 1/(composite-std), where composite-std is computed as the squareroot of the sum of variances associated with\n",
      "# 1. half the altitude error, 2. measurement error, and 3. number of measurements.  OK, the standard deviation for weighting based on the\n",
      "# number of measurements has nothing to do with the variability of the measured values.  Instead, the std value is computed as\n",
      "# 1/squareroot of the number of measurements.  Thus, wells with many msmts have less uncertainty (std) associated with them.\n",
      "# Altitude error was ultimately dropped because it was overly diluting weights for long-term wells that\n",
      "# had never been surveyed. That is, it was counter-acting the value of long-term records for wells in the GW network.\n",
      "\n",
      "# lev2['ini_weight'] = 1/(np.sqrt((lev2.alt_acy_va / 1.96)**2 + (lev2.cum_err / 1.96)**2 + (1/np.sqrt(lev2.n_msmts))**2))\n",
      "# lev2['ini_weight'] = 1/(np.sqrt((lev2.cum_err / 1.96)**2 + (1/np.sqrt(lev2.n_msmts))**2))\n",
      "lev2['ini_weight'] = 1/(np.sqrt((lev2.cum_err / 1.96)**2 + (1/np.power(lev2.n_msmts, 1./5))**2))  # try nth root to reduce effect of nmsmts\n",
      "\n",
      "lev2['group'] = 'NoGroup'\n",
      "condlist = [lev2['td']>=10*365.25, lev2['td']>=365.25, lev2['td']<365.25]\n",
      "choicelist = ['long-term', 'medium-term', 'short-term']\n",
      "lev2['group'] = np.select(condlist, choicelist,'OOPS')\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "# visualize distrib of some characteristics\n",
      "lev3['daypermsmt'] = lev3.td / lev3.n_msmts\n",
      "lev3['msmtperday'] = lev3.n_msmts / lev3.td\n",
      "lev3.hist(column=['weight', 'ini_weight', 'n_msmts', 'td', 'daypermsmt', 'msmtperday'], figsize=(12,10), bins = 50)\n",
      "lev3['ini_weight'].max()\n",
      "\n",
      "# play around with relationships to see if there is a better way to assign weights using an equation that incorporates\n",
      "# the number of measurements.\n",
      "lev3['test_weight'] = 1/(np.sqrt((lev2.cum_err / 1.96)**2 + ((10)/np.power(lev2.n_msmts, 1./2))**2))\n",
      "nmnts = lev3.n_msmts.tolist()\n",
      "tds = lev3.td.tolist()\n",
      "return_inter = lev3.daypermsmt.tolist()\n",
      "freq = lev3.msmtperday.tolist()\n",
      "neighs = lev3.neighbors.tolist()\n",
      "inis = lev3.ini_weight.tolist()\n",
      "wghts = lev3.weight.tolist()\n",
      "test = lev3.test_weight.tolist()\n",
      "plt.figure()\n",
      "#plt.scatter(tds, return_inter)\n",
      "#plt.scatter(tds, freq)\n",
      "#plt.scatter(tds, nmnts)\n",
      "#plt.scatter(nmnts, return_inter)  # many wells with lots of msmts over a short time.\n",
      "plt.scatter(nmnts, freq)  # good relationship between msmts/time and number msmts.  Some wells appear to have daily data!\n",
      "#plt.scatter(nmnts, tds)\n",
      "#plt.scatter(nmnts, inis)\n",
      "#plt.scatter(nmnts, test, c='k',marker='x')\n",
      "#plt.scatter(nmnts, wghts)\n",
      "#plt.scatter(neighs, wghts)\n",
      "#plt.scatter(neighs, nmnts, c='k',marker='x')\n",
      "plt.title('Initial weight vs number of measurements')\n",
      "plt.xlabel('number of measurements')\n",
      "plt.ylabel('Initial weight (before decluster)')\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Problem with equation approach is that n_msmts is sooo skewed.  Efforts to limit max weights deminishes \n",
      "# variability at low end.  Bins may work better.\n",
      "lev3.hist(['n_msmts'], bins=100)\n",
      "lev3.n_msmts.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# total days of observation a better metric for weights?  \n",
      "# No-- but use 'td' for groups for weight re-adjustment during calib process w/ 'weight-o-matic'.\n",
      "lev3.td.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lev3['group'] = 'NoGroup'\n",
      "condlist = [lev3['td']>=10*365.25, lev3['td']>=365.25, lev3['td']<365.25]\n",
      "choicelist = ['long-term', 'medium-term', 'short-term']\n",
      "lev3['group'] = np.select(condlist, choicelist,'OOPS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Percentiles of n_msmts'\n",
      "print '75%, 90%,  95%,  99%'\n",
      "print lev3.n_msmts.quantile(.75), lev3.n_msmts.quantile(.9), lev3.n_msmts.quantile(.95), lev3.n_msmts.quantile(.99)\n",
      "\n",
      "# specify percentiles by which to bin weights.\n",
      "percentiles = [.25, .75, .9]\n",
      "out, bins = pd.qcut(lev3.n_msmts, percentiles, retbins=True)\n",
      "bins  # number of measurements at each percentile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#weight by specified percentiles\n",
      "lev3['ini_weight'] = -1.0\n",
      "lev3['msmt_acy'] = -1.0\n",
      "#quartile_msmts = lev3['n_msmts'].describe()[['25%', '50%', '75%', 'max']].get_values()\n",
      "condlist = [lev3['n_msmts']>=bins[2], lev3['n_msmts']>=bins[1], lev3['n_msmts']>=bins[0], lev3['n_msmts']<bins[0]]\n",
      "choicelist = [.5, 1.0, 2.0, 4.0]  # \"95%CI vert accuracy estimated within +/- this value based on n_msmts\"\n",
      "lev3['msmt_acy'] = np.select(condlist, choicelist,-1.0)\n",
      "lev3['ini_weight'] = 1/(np.sqrt((lev3.cum_err / 1.96)**2 + (lev3.msmt_acy / 1.96)**2))\n",
      "lev3.hist(column=['msmt_acy', 'ini_weight'], figsize=(12,5))\n",
      "lev3.ini_weight.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Decluster by reducing weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# process data bins + 1 times.  First time, run on all points.  \n",
      "# Then loop on the bin values so adjust weight of fewer and fewer wells.\n",
      "geom = lev3['geometry'].tolist()\n",
      "print len(geom)\n",
      "xcoords = [geom[i].x for i,j in enumerate(geom)]\n",
      "ycoords = [geom[i].y for i,j in enumerate(geom)]\n",
      "xy = zip(xcoords, ycoords)\n",
      "pts = np.array(xy)\n",
      "w_pts = ps.threshold_binaryW_from_array(pts, decluster_distance)  # count neighbors within \"dist\" of each point\n",
      "ws = [w_pts.weights[i] for i,j in enumerate(w_pts)]  # pull out a list of weights for each pt. The list contains a 1 for each neighbor\n",
      "ws = np.array(ws)\n",
      "sm = [np.sum(i, axis=0) for i in ws]  # sum-up all of the weights (n_neighbors) for each point and convert to a vector array\n",
      "sm = np.array(sm)\n",
      "print sm.size, '  ', ws.size, '\\n'\n",
      "lev3['neighbors'] = sm +1  # +1 to count all pts w/in threshold, including self.  Also avoids divid-by-zero problems.\n",
      "\n",
      "for q, val in enumerate(bins):\n",
      "    geom = lev3.loc[lev3['n_msmts']>=val, :]['geometry'].tolist()\n",
      "    print len(geom), '  ', val\n",
      "    \n",
      "    print 'Reducing weight for wells that are clustered within {} (ft or m) of wells w/ same or better n_msmt accy'.format(decluster_distance)\n",
      "    xcoords = [geom[i].x for i,j in enumerate(geom)]\n",
      "    ycoords = [geom[i].y for i,j in enumerate(geom)]\n",
      "    xy = zip(xcoords, ycoords)\n",
      "    pts = np.array(xy)\n",
      "    w_pts = ps.threshold_binaryW_from_array(pts, decluster_distance)  # count neighbors within \"dist\" of each point\n",
      "    ws = [w_pts.weights[i] for i,j in enumerate(w_pts)]  # pull out a list of weights for each pt. The list contains a 1 for each neighbor\n",
      "    ws = np.array(ws)\n",
      "    sm = [np.sum(i, axis=0) for i in ws]  # sum-up all of the weights (n_neighbors) for each point and convert to a vector array\n",
      "    sm = np.array(sm)\n",
      "    print sm.size, '  ', ws.size, '\\n'\n",
      "    lev3['neighbors'][lev3['n_msmts']>=val] = sm +1  # +1 to count all pts w/in threshold, including self.  Also avoids divid-by-zero problems.\n",
      "# The warning is OK.  Want to apply it to a view."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check that sub-setting neighbors worked.  Many of these wells used to have 20+ neighbors.\n",
      "print sm\n",
      "lev3.loc[lev3['n_msmts']>=val, ['neighbors']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reduce weight by number of neighbors (in same weight bin).\n",
      "lev3['weight'] = lev3['ini_weight'] / lev3['neighbors']\n",
      "lev3.hist(column=['msmt_acy', 'ini_weight', 'neighbors', 'weight'], figsize=(12,10))\n",
      "lev3.weight.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Write mod2obs files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Format for Mod2Obs, including unique 10-digit IDs\n",
      "# Setup for mod2obs1; could use regular mod2obs if remove \"a\" values from ofp.write, below.\n",
      "ofp = open(driverfile, 'w')\n",
      "ofp.write('{1}{0}{2}{0}{3}{0}{4}{0}a{0}{5}{0}\\\n",
      "{6}{0}{7:<10.2e}{0}{8}{0}{9}{0}{10}{0}\\\n",
      "{11}{0}{12:<10.2e}{0}{13}{0}a{0}'.format(newline,\n",
      "                                 spc_file,\n",
      "                                 bore_coords_file,\n",
      "                                 bore_list_file,\n",
      "                                 well_sample_file,\n",
      "                                 hds_file,\n",
      "                                 flow_or_trans,\n",
      "                                 inactive_thresh,\n",
      "                                 time_units,\n",
      "                                 ss_arbitrary_date,\n",
      "                                 ss_arbitrary_time,\n",
      "                                 nlay,\n",
      "                                 extrapolation_limit,\n",
      "                                 output_file))\n",
      "ofp.close()\n",
      "\n",
      "lev3.to_csv(bore_coords_file, columns=[u'labels', u'x', u'y', u'layer'], sep=' ', index='', header='')\n",
      "lev3.to_csv(bore_list_file, columns=[u'labels'], sep=' ', index='', header='')\n",
      "lev3.to_csv(well_sample_file, columns=[u'labels', u'date', u'time', u'target'],\n",
      "            sep=' ', float_format='%.3f', index='', header='')\n",
      "# make ins file\n",
      "names_upper = [i.upper() for i in list(lev3['labels'])]\n",
      "lev3['Name'] = names_upper\n",
      "lev3['w'] = 'w'\n",
      "lev3['hashname'] = '#' + lev3['Name'] + '#'\n",
      "lev3['bangname'] = '!' + lev3['Name'] + '!'\n",
      "## write the ins file\n",
      "open(insfile, 'w').write('pif #\\n')\n",
      "with open(insfile, 'a') as file:\n",
      "    lev3.to_csv(file, columns=[u'hashname',u'w',u'w',u'w',u'bangname'],\n",
      "                  sep=' ', index='', header='')\n",
      "# make pst file parts\n",
      "lev3.to_csv(pest_head_obs_sect_file, columns=[u'Name',u'target',u'weight',u'group'],\n",
      "                  sep=' ', float_format='%.3f', index='', header='', line_terminator=newline)\n",
      "print '\\nSuccessful completion.  Mod2Obs files are located here:\\n' \\\n",
      "      '{}'.format(outpath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Write-out a shapefile"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reduce field names and write-out shapefile\n",
      "lev3['dec_long'] = lev3['dec_long_va']\n",
      "lev3['coord_acy'] = lev3['coord_acy_cd']\n",
      "lev3['nat_aqfr'] = lev3['nat_aqfr_cd']\n",
      "lev3['aqfr_type'] = lev3['aqfr_type_cd']\n",
      "lev3['well_depth'] = lev3['well_depth_va']\n",
      "lev3 = lev3.drop(['dec_long_va', 'coord_acy_cd', 'nat_aqfr_cd', 'aqfr_type_cd', 'well_depth_va',], axis=1)\n",
      "lev3 = lev3.drop(['keep', 'dup', 'date', 'time', 'w', 'hashname', 'bangname'], axis=1)\n",
      "print '\\nA shapefile of the final targets is located here:\\n' \\\n",
      "      '{}'.format(outshape)\n",
      "lev3.to_file(outshape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}